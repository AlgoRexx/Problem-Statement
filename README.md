# **FastAPI Web Scraper with Vector Search (ChromaDB + Hugging Face Models)**  

### **ğŸ“Œ Overview**  
This FastAPI-based server extracts and processes content from any external URL, generates vector embeddings, and provides a queryable system using **ChromaDB** and **Hugging Face models**.  

### **ğŸš€ Features**  
âœ… **Scrape Any Website** â€“ Extracts text from any URL (supports Depth = 1 recursion).  
âœ… **Stores Data in ChromaDB** â€“ Uses **`all-MiniLM-L6-v2`** embeddings for efficient search.  
âœ… **AI-Powered Querying** â€“ Retrieves relevant text and generates responses using **BART (`facebook/bart-large-cnn`)**.  
âœ… **Auto-Deletes Database** â€“ Removes ChromaDB after execution for cleanup.  
âœ… **Optimized for Production** â€“ Works seamlessly on **local machines or cloud servers**.  

---

## **ğŸ“‚ Project Structure**  
```
ğŸ“ fastapi-web-scraper
â”‚â”€â”€ assign.py                 # FastAPI server with scraping & query endpoints
â”‚â”€â”€ requirements.txt        # Dependencies for easy installation
â”‚â”€â”€ README.md               # Documentation (this file)
â”‚â”€â”€ .gitignore              # Ignore database & environment files
â””â”€â”€ chroma_db/              # Vector database (created at runtime)
```

---

## **ğŸ“Œ Installation & Setup**  

### **1ï¸âƒ£ Clone the Repository**
```bash
git clone https://github.com/your-username/fastapi-web-scraper.git
cd fastapi-web-scraper
```

### **2ï¸âƒ£ Ensure Python 3.9.7 is Installed (Using Conda)**  
This project requires Python 3.9.7, so set up a Conda environment.  

ğŸ”¹ Step 1: Create a Conda Environment
```bash
conda create --name fastapi_scraper python=3.9.7 -y
```
ğŸ”¹ Step 2: Activate the Conda Environment
```bash
conda activate fastapi_scraper
```

### **3ï¸âƒ£ Install Dependencies**  
```bash
pip install -r requirements.txt
```

---

## **ğŸš€ Running the FastAPI Server**
Instead of using Uvicorn, start the server with:  
```bash
sudo python assign.py
```
âœ… This ensures the server runs with the required permissions.  

ğŸ”— **Visit the API documentation:**  
ğŸ“œ Swagger UI â†’ [http://127.0.0.1:8000/docs](http://127.0.0.1:8000/docs)  
ğŸ“œ ReDoc UI â†’ [http://127.0.0.1:8000/redoc](http://127.0.0.1:8000/redoc)  

---

## **ğŸ›  API Endpoints**  

### **1ï¸âƒ£ `/url-parser` (POST) â€“ Scrape a Website**  
**ğŸ”¹ Description:**  
Extracts text from a webpage (including Depth = 1 links), converts it into vector embeddings, and stores it in **ChromaDB**.  

**ğŸ”¹ Example Request (JSON Body):**
```json
{
  "url": "https://en.wikipedia.org/wiki/Artificial_intelligence"
}
```
**ğŸ”¹ Example Response:**  
```json
{
  "message": "âœ… Web scraping is completed successfully!",
  "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
  "chunks_stored": 10
}
```

**ğŸ”¹ cURL Command:**  
```bash
curl -X 'POST' 'http://127.0.0.1:8000/url-parser' \
     -H 'Content-Type: application/json' \
     -d '{"url": "https://en.wikipedia.org/wiki/Artificial_intelligence"}'
```

**ğŸ”¹ Postman Request:**  
1. Open **Postman**  
2. **Set method to `POST`**  
3. **Enter URL:**  
   ```
   http://127.0.0.1:8000/url-parser
   ```
4. **Go to "Body" tab** â†’ Select **raw** â†’ Choose **JSON** format.
5. **Paste this JSON:**
   ```json
   {
     "url": "https://en.wikipedia.org/wiki/Artificial_intelligence"
   }
   ```
6. Click **Send**

---

### **2ï¸âƒ£ `/query` (POST) â€“ Search & Get AI Response**  
**ğŸ”¹ Description:**  
Searches stored embeddings for relevant text and generates an **AI-powered response** based on context.  

**ğŸ”¹ Example Request (JSON Body):**  
```json
{
  "query": "What is Artificial Intelligence?"
}
```
**ğŸ”¹ Example Response:**  
```json
{
  "query": "What is Artificial Intelligence?",
  "retrieved_context": "Artificial intelligence (AI) is the intelligence of machines or software...",
  "response": "Artificial intelligence refers to the simulation of human intelligence in machines..."
}
```

**ğŸ”¹ cURL Command:**  
```bash
curl -X 'POST' 'http://127.0.0.1:8000/query' \
     -H 'Content-Type: application/json' \
     -d '{"query": "What is Artificial Intelligence?"}'
```

**ğŸ”¹ Postman Request:**  
1. Open **Postman**  
2. **Set method to `POST`**  
3. **Enter URL:**  
   ```
   http://127.0.0.1:8000/query
   ```
4. **Go to "Body" tab** â†’ Select **raw** â†’ Choose **JSON** format.
5. **Paste this JSON:**
   ```json
   {
     "query": "What is Artificial Intelligence?"
   }
   ```
6. Click **Send**

---

## **ğŸ“Œ Import Postman Collection**
To make testing easier, you can **import a Postman collection** instead of manually setting everything up.

1. Open **Postman**  
2. Click **"Import"**  
3. Select **"Raw text"** and paste this JSON:  
```json
{
  "info": {
    "name": "FastAPI Scraper API",
    "_postman_id": "abcd-1234",
    "schema": "https://schema.getpostman.com/json/collection/v2.1.0/collection.json"
  },
  "item": [
    {
      "name": "Scrape Website",
      "request": {
        "method": "POST",
        "header": [{ "key": "Content-Type", "value": "application/json" }],
        "url": { "raw": "http://127.0.0.1:8000/url-parser" },
        "body": { "mode": "raw", "raw": "{ \"url\": \"https://en.wikipedia.org/wiki/Artificial_intelligence\" }" }
      }
    },
    {
      "name": "Query Stored Data",
      "request": {
        "method": "POST",
        "header": [{ "key": "Content-Type", "value": "application/json" }],
        "url": { "raw": "http://127.0.0.1:8000/query" },
        "body": { "mode": "raw", "raw": "{ \"query\": \"What is Artificial Intelligence?\" }" }
      }
    }
  ]
}
```
4. Click **Import**, and both endpoints will be ready to use!
